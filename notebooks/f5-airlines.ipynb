{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from  pandasgui import show\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103904 entries, 0 to 103903\n",
      "Data columns (total 25 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   Unnamed: 0                         103904 non-null  int64  \n",
      " 1   id                                 103904 non-null  int64  \n",
      " 2   Gender                             103904 non-null  object \n",
      " 3   Customer Type                      103904 non-null  object \n",
      " 4   Age                                103904 non-null  int64  \n",
      " 5   Type of Travel                     103904 non-null  object \n",
      " 6   Class                              103904 non-null  object \n",
      " 7   Flight Distance                    103904 non-null  int64  \n",
      " 8   Inflight wifi service              103904 non-null  int64  \n",
      " 9   Departure/Arrival time convenient  103904 non-null  int64  \n",
      " 10  Ease of Online booking             103904 non-null  int64  \n",
      " 11  Gate location                      103904 non-null  int64  \n",
      " 12  Food and drink                     103904 non-null  int64  \n",
      " 13  Online boarding                    103904 non-null  int64  \n",
      " 14  Seat comfort                       103904 non-null  int64  \n",
      " 15  Inflight entertainment             103904 non-null  int64  \n",
      " 16  On-board service                   103904 non-null  int64  \n",
      " 17  Leg room service                   103904 non-null  int64  \n",
      " 18  Baggage handling                   103904 non-null  int64  \n",
      " 19  Checkin service                    103904 non-null  int64  \n",
      " 20  Inflight service                   103904 non-null  int64  \n",
      " 21  Cleanliness                        103904 non-null  int64  \n",
      " 22  Departure Delay in Minutes         103904 non-null  int64  \n",
      " 23  Arrival Delay in Minutes           103594 non-null  float64\n",
      " 24  satisfaction                       103904 non-null  object \n",
      "dtypes: float64(1), int64(19), object(5)\n",
      "memory usage: 19.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_data = pd.read_csv('../data/airline_passenger_satisfaction.csv')\n",
    "print(train_data.info())\n",
    "# primeros_100_registros = train_data.iloc[:100]\n",
    "# test_data = pd.DataFrame(primeros_100_registros)\n",
    "primer_registro = train_data.iloc[0]\n",
    "test_data = pd.DataFrame([primer_registro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos un arreglo con los nombres de las variables segun su tipo\n",
    "imputer_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in train_data.columns if train_data[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean' )\n",
    "imputer.fit(train_data[imputer_cols])\n",
    "train_data[imputer_cols] = imputer.transform(train_data[imputer_cols])\n",
    "test_data[imputer_cols] = imputer.transform(test_data[imputer_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completamos valores nulos  en las columnas categoricas con la moda\n",
    "def fill_null_with_mode(column, train_df, test_df):\n",
    "    moda = train_df[column].mode().iloc[0]\n",
    "    train_df[column] = train_df[column].fillna(moda)\n",
    "    test_df[column] = test_df[column].fillna(moda)\n",
    "\n",
    "# Aplicar la función de llenado de valores nulos\n",
    "fill_null_with_mode(categorical_cols, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparamos los datos para dividirlos\n",
    "train_data.drop([\"Unnamed: 0\" ,\"id\"] , axis = 1 ,inplace = True)\n",
    "test_data.drop([\"Unnamed: 0\" ,\"id\",\"satisfaction\"] , axis = 1 ,inplace = True)\n",
    "# TODO no estoy seguro si debo elimiar satisfaction de test\n",
    "X = train_data.drop(\"satisfaction\" , axis =1 )\n",
    "y = train_data[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n",
      "['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
      "[]\n",
      "Training set shape: (103904, 24)\n"
     ]
    }
   ],
   "source": [
    "# encoding y escaling\n",
    "\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "print(numerical_cols)\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "print(categorical_cols)\n",
    "boolean_cols = [cname for cname in X.columns if X[cname].dtype == 'bool']\n",
    "print(boolean_cols)\n",
    "\n",
    "# Scale numerical data to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "# One-hot encode categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore',sparse=False))])\n",
    "\n",
    "# Combine preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "# Apply preprocessing\n",
    "X = ct.fit_transform(X)\n",
    "test_data = ct.transform(test_data)\n",
    "\n",
    "# Print new shape\n",
    "print('Training set shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,train_size=0.8,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': range(20, 101, 20),\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "list(param_grid['n_estimators'])\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scoring, cv=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 90 candidates, totalling 180 fits\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=40; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=60; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=60; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=80; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=80; total time=   4.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=100; total time=   6.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=100; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=20; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=20; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=80; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=80; total time=   6.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=100; total time=   7.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=20; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=20; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=40; total time=   5.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=40; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=60; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=60; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=100; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=100; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=20; total time=   1.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.8s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=80; total time=   3.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=80; total time=   3.1s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=60; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=60; total time=   3.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=80; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=80; total time=   4.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=60; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=60; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=80; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=80; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   6.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=40; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=60; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=60; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=80; total time=   4.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=80; total time=   4.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   6.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=40; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=40; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=60; total time=   5.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=60; total time=   5.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=80; total time=   6.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=80; total time=   6.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=100; total time=   8.5s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=100; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=20; total time=   1.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=20; total time=   1.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.9s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=80; total time=   8.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=80; total time=   6.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=20; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=20; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=40; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=40; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=60; total time=   4.9s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=60; total time=   4.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=100; total time=   7.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=100; total time=   7.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=20; total time=   1.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=80; total time=   2.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=60; total time=   2.7s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=60; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=80; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=80; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=20; total time=   0.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=20; total time=   0.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=40; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=80; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=40; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=80; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=80; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score: 0.9578095287124689\n",
      "Testing accuracy: 0.9617920215581541\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    " \n",
    "# Re-train the model with the best hyperparameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    " \n",
    "# Test the model with the best hyperparameters on the testing data\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(\"Testing accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability: 0.4336235371111795\n"
     ]
    }
   ],
   "source": [
    "# Crear el pipeline con los parametros del grid search\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestClassifier(criterion = 'gini' ,max_depth= None, min_samples_split=2, n_estimators=100))\n",
    "])\n",
    "\n",
    "# Realizar la validación cruzada y obtener las probabilidades y los scores\n",
    "proba_predictions = cross_val_predict(my_pipeline, X_train, y_train, cv=5, method='predict_proba')\n",
    "accuracy_scores = cross_val_predict(my_pipeline, X_train, y_train, cv=5, method='predict')\n",
    "\n",
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisión\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "# print(\"Average accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(criterion = 'gini', max_depth= None, min_samples_split=2, n_estimators=100)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral or dissatisfied'], dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability: 0.4335077571604558\n",
      "Average accuracy: 0.9623017400677548\n",
      "Matriz de Confusión:\n",
      " [[11505   271]\n",
      " [  550  8455]]\n",
      "Precisión: 0.96\n",
      "Recall: 0.96\n",
      "F1-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisión\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "#Matriz\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcula la precisión\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average probability**  \n",
    "Esto nos indica que de la cantidad de clientes satisfechos (satisfied) hay un 43.35% del total general , por lo tanto lo restante serian los que estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Average accuracy**  \n",
    "Aqui podemos observar que en cuanto a predicciones correctas, es decir que nuestro modelo cuenta con un 96.15% de acertividad en cuanto a las pruebas realizadas.\n",
    "\n",
    "**Matriz de confusión**  \n",
    "La matriz de confusión es la variable que nos permite determinar la capacidad del modelo para evitar clasificar incorrectamente las instancias negativas como positivas. A diferencia de la precisión uqe nos da una vision general de la acertividad , la matriz de confusión se utiliza para saber la categorizacion de las predicciones.\n",
    "\n",
    "En esta métrica nos indica cuantos verdaderos positivos hay (11510) es decir que la mayoria de resultados aqui estan estado \"satisfied\"; Falsos positivos aqui se puede ver que el modelo interpretó 266 clientes que en realidad estan en estado  \"neutral or dissatisfied\" pero que el modelo tomó como \"satisfied\"; En cuanto a 579 , nos indica que el modelo interpreta estos datos como falsos negativos , que quiere decir que en realidad estos clientes estan como \"satisfied\" pero el modelo los tomó como \"neutral or dissatisfied\"; por último 8426 que significa que aqui estan los verdaderos negativos , que aqui todos los clientes estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Precisión**  \n",
    "Esto nos indica que de toda la base de datos el 96% es acertado con respecto a predicciones positivas, a diferencia de ka atriz de confusión , la precisión no tiene en cuenta estos falsos positivos y se centra en la proporción de predicciones positivas correctas en relación con todas las predicciones positivas. \n",
    "\n",
    "**Recall**  \n",
    "El recall responde a la pregunta: \"De todos los casos positivos reales, ¿cuántos de ellos el modelo fue capaz de identificar correctamente?\". Es una métrica importante en problemas donde la detección de todos los casos positivos es crítica, como en la detección temprana de enfermedades o la identificación de fraudes. En este caso se puede determinar que el 96% de de los casos verdaderos positivos es correctamente identificado por el modelo.\n",
    "\n",
    "**F1-Score**  \n",
    "El F1-Score es útil cuando deseamos tener un equilibrio entre la precisión y el recall. Proporciona una puntuación que combina ambas métricas y es útil cuando ninguna de las dos métricas por sí sola es suficiente. En nuestro caso indica que está cerca al 1.0 entonces indica un buen equilibrio entre la precisión y el recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la importancia de las características\n",
    "importancia_caracteristicas = classifier.feature_importances_\n",
    "\n",
    "# Puedes imprimir la importancia de cada característica\n",
    "for i, importancia in enumerate(importancia_caracteristicas):\n",
    "    print(f'Característica {i}: {importancia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapea las etiquetas a valores binarios\n",
    "y_true_binary = [1 if label == \"satisfied\" else 0 for label in y_test]\n",
    "\n",
    "# Obtén las probabilidades de predicción del modelo (por ejemplo, un clasificador de bosque aleatorio)\n",
    "probs = classifier.predict_proba(X_test)  # X_test son las características de prueba\n",
    "\n",
    "# Calcula la curva ROC especificando pos_label=1\n",
    "fpr, tpr, umbrales = roc_curve(y_true_binary, probs[:, 1], pos_label=1)\n",
    "\n",
    "# Calcula el área bajo la curva ROC (AUC-ROC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Grafica la curva ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de curva ROC \n",
    "\n",
    "En la curva de ROC podremos notar como el modelo tiene un 0.99 de acertividad a la hora de clasificar, sabiendo distinguir correctamente entre valores correctos e incorrectos con respecto al humbral. La curva de satisfechos y la curva de no satisfechos no se superponen casi en lo absoluto, permitiendo al modelo una facil identificacion y teniendo un minimo error al distinguir entre uno o el otro. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F5-Airlines-CuSJDrj7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
